{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T17:42:16.966745Z",
     "start_time": "2025-06-01T17:42:12.684588Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "from multiprocessing import freeze_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score,\n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from monai.data import Dataset\n",
    "from models import resnet\n",
    "from datasets.ADNI import ADNI, ADNI_transform\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载配置文件\n",
    "def load_config(path=\"config/config.json\"):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 配置类\n",
    "class Config:\n",
    "    def __init__(self, d):\n",
    "        for k, v in d.items(): setattr(self, k, v)\n",
    "        self.weight_decay = getattr(self, 'weight_decay', 1e-4)\n",
    "        self.dropout_rate = getattr(self, 'dropout_rate', 0.5)\n",
    "        self.n_splits = getattr(self, 'n_splits', 5)\n",
    "        self.print_config()\n",
    "\n",
    "    def print_config(self):\n",
    "        print(\"Configuration Parameters:\\n\" + \"=\" * 40)\n",
    "        for k, v in vars(self).items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        print(\"=\" * 40)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T17:42:19.927634Z",
     "start_time": "2025-06-01T17:42:19.911527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_model(model_type='resnet', model_depth=50,\n",
    "                   input_W=224, input_H=224, input_D=224,\n",
    "                   resnet_shortcut='B',\n",
    "                   pretrain_path='config/pretrain/resnet_50_23dataset.pth',\n",
    "                   nb_class=2,  # 修改为2分类输出\n",
    "                   dropout_rate=0.5,\n",
    "                   device=torch.device('cpu')):\n",
    "    assert model_type == 'resnet'\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    fn = {\n",
    "        10: resnet.resnet10, 18: resnet.resnet18, 34: resnet.resnet34,\n",
    "        50: resnet.resnet50, 101: resnet.resnet101,\n",
    "        152: resnet.resnet152, 200: resnet.resnet200\n",
    "    }[model_depth]\n",
    "\n",
    "    net = fn(\n",
    "        sample_input_W=input_W, sample_input_H=input_H, sample_input_D=input_D,\n",
    "        shortcut_type=resnet_shortcut, no_cuda=True, num_seg_classes=1\n",
    "    )\n",
    "\n",
    "    fc_in = {10: 256, 18: 512, 34: 512, 50: 2048, 101: 2048, 152: 2048, 200: 2048}[model_depth]\n",
    "    net.conv_seg = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool3d((1, 1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(p=dropout_rate),\n",
    "        nn.Linear(fc_in, nb_class)  # 输出维度与nb_class=2匹配\n",
    "    )\n",
    "\n",
    "    net.to(device)\n",
    "    sd = net.state_dict()\n",
    "    if os.path.isfile(pretrain_path):\n",
    "        ckpt = torch.load(pretrain_path, map_location=device)\n",
    "        state = ckpt.get('state_dict', ckpt)\n",
    "        pd = {k: v for k, v in state.items() if k in sd}\n",
    "        sd.update(pd)\n",
    "        net.load_state_dict(sd)\n",
    "        print(\"Loaded pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"[Warning] no pretrained file at {pretrain_path}\")\n",
    "    return net"
   ],
   "id": "780a534d899538fb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T17:42:24.528Z",
     "start_time": "2025-06-01T17:42:24.493631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred, y_score):\n",
    "    return {\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'auc': roc_auc_score(y_true, y_score),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'cm': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# 加载数据（首次运行时执行，后续调试可注释）\n",
    "cfg = Config(load_config())\n",
    "dataset = ADNI(cfg.label_file, cfg.mri_dir, cfg.task, cfg.augment).data_dict\n",
    "tr_val, test_data = train_test_split(dataset, test_size=0.2, random_state=42, stratify=[d['label'] for d in dataset])\n",
    "labels = [d['label'] for d in tr_val]  # 用于分层交叉验证"
   ],
   "id": "51d92027ce5543ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Parameters:\n",
      "========================================\n",
      "dataroot: C:\\Users\\dongz\\Desktop\\adni_dataset\n",
      "label_file: C:\\Users\\dongz\\Desktop\\adni_dataset\\ADNI_902.csv\n",
      "mri_dir: C:\\Users\\dongz\\Desktop\\adni_dataset\\MRI_GM_113_137_113\n",
      "task: ADCN\n",
      "augment: False\n",
      "split_ratio: 0.2\n",
      "seed: 42\n",
      "num_epochs: 50\n",
      "batch_size: 16\n",
      "lr: 1e-05\n",
      "checkpoint_dir: ./checkpoints\n",
      "log_file: training_log1.csv\n",
      "model_type: resnet\n",
      "model_depth: 50\n",
      "input_W: 80\n",
      "input_H: 98\n",
      "input_D: 80\n",
      "resnet_shortcut: B\n",
      "pretrain_path: config/pretrain/resnet_50_23dataset.pth\n",
      "nb_class: 2\n",
      "n_splits: 2\n",
      "dropout_rate: 0.5\n",
      "weight_decay: 0.0001\n",
      "========================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T17:42:54.249625Z",
     "start_time": "2025-06-01T17:42:54.229320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    cfg = Config(load_config())\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # 初始化日志（仅首次运行时创建CSV，后续追加）\n",
    "    writer = SummaryWriter(cfg.checkpoint_dir)\n",
    "    csv_path = os.path.join(cfg.checkpoint_dir, 'cv_results.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            csv.writer(f).writerow(\n",
    "                ['fold', 'epoch',\n",
    "                 'tr_acc', 'tr_auc', 'tr_loss',\n",
    "                 'vl_acc', 'vl_auc', 'vl_loss', 'lr']\n",
    "            )\n",
    "    \n",
    "    # 分层交叉验证（可单独调试某一折）\n",
    "    kf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(tr_val, labels), 1):\n",
    "        print(f\"\\n=== Fold {fold}/{cfg.n_splits} ===\")\n",
    "        train_data = [tr_val[i] for i in train_idx]\n",
    "        val_data = [tr_val[i] for i in val_idx]\n",
    "        \n",
    "        # 数据预处理（可单独调试数据转换）\n",
    "        tf_tr, tf_vt = ADNI_transform(augment=cfg.augment)\n",
    "        ds_tr = Dataset(data=train_data, transform=tf_tr)\n",
    "        ds_vl = Dataset(data=val_data, transform=tf_vt)\n",
    "        loader_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        loader_vl = DataLoader(ds_vl, batch_size=cfg.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        \n",
    "        # 模型与优化器初始化（可调试模型结构）\n",
    "        model = generate_model(\n",
    "            model_type=cfg.model_type, model_depth=cfg.model_depth,\n",
    "            input_W=cfg.input_W, input_H=cfg.input_H, input_D=cfg.input_D,\n",
    "            resnet_shortcut=cfg.resnet_shortcut,\n",
    "            pretrain_path=cfg.pretrain_path,\n",
    "            nb_class=2,\n",
    "            dropout_rate=cfg.dropout_rate,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # 类别权重与损失函数（可调试权重计算）\n",
    "        class_counts = np.bincount([d['label'] for d in train_data])\n",
    "        class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float32).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        \n",
    "        # 学习率调度器（可调试超参数）\n",
    "        warmup_epochs = max(1, min(10, int(cfg.num_epochs * 0.1)))\n",
    "        total_epochs = cfg.num_epochs\n",
    "        cosine_epochs = total_epochs - warmup_epochs\n",
    "        min_lr = cfg.lr * 1e-4\n",
    "        warmup_sched = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs)\n",
    "        cosine_sched = CosineAnnealingLR(optimizer, T_max=cosine_epochs, eta_min=min_lr)\n",
    "        scheduler = SequentialLR(optimizer, schedulers=[warmup_sched, cosine_sched], milestones=[warmup_epochs])\n",
    "        \n",
    "        # 训练循环（可逐epoch调试）\n",
    "        best_metric = -np.inf\n",
    "        for epoch in range(1, cfg.num_epochs + 1):\n",
    "            t0 = time.time()\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            y_true, y_pred, y_score = [], [], []\n",
    "            \n",
    "            # 训练批次循环（可调试单批次数据）\n",
    "            for batch in loader_tr:\n",
    "                x = batch['MRI'].to(device)\n",
    "                y = batch['label'].to(device).squeeze().long()\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "                loss_sum += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 记录预测结果（可调试预测逻辑）\n",
    "                probs = torch.softmax(out, 1)[:, 1].detach().cpu().numpy()\n",
    "                preds = out.argmax(1).detach().cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_score.extend(probs)\n",
    "                y_pred.extend(preds)\n",
    "            \n",
    "            # 训练集指标（可单独计算指标）\n",
    "            tr_metrics = calculate_metrics(y_true, y_pred, y_score)\n",
    "            tr_loss = loss_sum / len(loader_tr)\n",
    "            \n",
    "            # 验证集评估（可单独调试验证逻辑）\n",
    "            model.eval()\n",
    "            v_true, v_pred, v_score = [], [], []\n",
    "            vl_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in loader_vl:\n",
    "                    x = batch['MRI'].to(device)\n",
    "                    y = batch['label'].to(device).squeeze().long()\n",
    "                    out = model(x)\n",
    "                    loss = nn.CrossEntropyLoss()(out, y)\n",
    "                    vl_loss += loss.item()\n",
    "                    \n",
    "                    probs = torch.softmax(out, 1)[:, 1].cpu().numpy()\n",
    "                    v_pred.extend(out.argmax(1).cpu().numpy())\n",
    "                    v_true.extend(y.cpu().numpy())\n",
    "                    v_score.extend(probs)\n",
    "            \n",
    "            vl_metrics = calculate_metrics(v_true, v_pred, v_score)\n",
    "            vl_loss = vl_loss / len(loader_vl)\n",
    "            lr_now = scheduler.get_last_lr()[0]\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 日志记录（可注释以加速调试）\n",
    "            writer.add_scalar(f'fold{fold}/train/acc', tr_metrics['acc'], epoch)\n",
    "            with open(csv_path, 'a', newline='') as f:\n",
    "                csv.writer(f).writerow([fold, epoch, *tr_metrics.values(), tr_loss, *vl_metrics.values(), vl_loss, lr_now])\n",
    "            \n",
    "            print(f\"Fold{fold} Ep{epoch:03d} | tr_acc={tr_metrics['acc']:.4f} vl_acc={vl_metrics['acc']:.4f} lr={lr_now:.7f}\")\n",
    "            \n",
    "            # 模型保存（调试时可注释）\n",
    "            current_metric = 0.3 * vl_metrics['auc'] + 0.7 * vl_metrics['acc']\n",
    "            if current_metric > best_metric:\n",
    "                best_metric = current_metric\n",
    "                torch.save({...}, os.path.join(cfg.checkpoint_dir, f\"best_fold{fold}.pth\"))\n",
    "    \n",
    "    writer.close()"
   ],
   "id": "8f23a75d25df795c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-01T17:43:08.188696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_models(checkpoint_dir, test_data):\n",
    "    \"\"\"测试函数（可单独调试测试逻辑）\"\"\"\n",
    "    cfg = Config(load_config())\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    test_transforms = ADNI_transform(augment=False)[1]\n",
    "    test_ds = Dataset(data=test_data, transform=test_transforms)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "    \n",
    "    # 测试循环（可调试单模型测试）\n",
    "    all_fold_probs = []\n",
    "    all_fold_labels = []\n",
    "    for fold in range(1, cfg.n_splits + 1):\n",
    "        model = generate_model(..., device=device)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"best_fold{fold}.pth\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch['MRI'].to(device)\n",
    "                y = batch['label'].squeeze().cpu().numpy()\n",
    "                out = model(x)\n",
    "                probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "                all_fold_probs.extend(probs)\n",
    "                all_fold_labels.extend(y)\n",
    "    \n",
    "    # 绘制ROC曲线（可单独调试可视化）\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    fpr, tpr, _ = roc_curve(all_fold_labels, all_fold_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'Mean ROC (AUC={roc_auc:.2f})')\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'test_roc_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "# 主程序（调试时可分阶段运行）\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()  # Windows系统需要，Linux/macOS可注释\n",
    "    train()  # 训练代码（首次运行时执行）\n",
    "    # test_models(cfg.checkpoint_dir, test_data)  # 测试代码（训练完成后执行）\n",
    "    pass  # 调试时先注释主程序，手动调用函数"
   ],
   "id": "c583520126476a34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Parameters:\n",
      "========================================\n",
      "dataroot: C:\\Users\\dongz\\Desktop\\adni_dataset\n",
      "label_file: C:\\Users\\dongz\\Desktop\\adni_dataset\\ADNI_902.csv\n",
      "mri_dir: C:\\Users\\dongz\\Desktop\\adni_dataset\\MRI_GM_113_137_113\n",
      "task: ADCN\n",
      "augment: False\n",
      "split_ratio: 0.2\n",
      "seed: 42\n",
      "num_epochs: 50\n",
      "batch_size: 16\n",
      "lr: 1e-05\n",
      "checkpoint_dir: ./checkpoints\n",
      "log_file: training_log1.csv\n",
      "model_type: resnet\n",
      "model_depth: 50\n",
      "input_W: 80\n",
      "input_H: 98\n",
      "input_D: 80\n",
      "resnet_shortcut: B\n",
      "pretrain_path: config/pretrain/resnet_50_23dataset.pth\n",
      "nb_class: 2\n",
      "n_splits: 2\n",
      "dropout_rate: 0.5\n",
      "weight_decay: 0.0001\n",
      "========================================\n",
      "Using device: cpu\n",
      "\n",
      "=== Fold 1/2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongz\\Desktop\\Multimodal_AD\\models\\resnet.py:173: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "C:\\Users\\dongz\\AppData\\Local\\Temp\\ipykernel_27976\\452330506.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pretrain_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e4f7e74e58c1b4c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
